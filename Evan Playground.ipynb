{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "with open(path + 'english_no_pad_sorted_50k.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'german_no_pad_sorted_50k.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x00000266D0D69978>\n"
     ]
    }
   ],
   "source": [
    "print(x.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(german['train'])):\n",
    "    german['train'][i] = torch.LongTensor(german['train'][i]).cuda()\n",
    "    english['train'][i] = torch.LongTensor(english['train'][i]).cuda()\n",
    "    \n",
    "for i in range(len(german['dev'])):\n",
    "    german['dev'][i] = torch.LongTensor(german['dev'][i]).cuda()\n",
    "    english['dev'][i] = torch.LongTensor(english['dev'][i]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-Based Batching Method\n",
    "\n",
    "Feed in the data from the preprocessed set, it should be sorted from shortest sentence to longest sentence without any padding on the sentences. The *get_batches* function will be used to create all of the batches for training, the output is a number of batches of varying dimensions, which is based on the batch size. Below is an example.\n",
    "\n",
    "#### Example, batch-size=14\n",
    "- Given the first few sentences from the dataset (sorted):\n",
    "    - [2, 84, 3]      (length 3)\n",
    "    - [2, 102, 3]     (length 3)\n",
    "    - [2, 63, 3]      (length 3)\n",
    "    - [2, 84, 21, 3]  (length 4)\n",
    "    - [2, 91, 123, 3] (length 4)\n",
    "    \n",
    "- We will fill up the batches based on the number of tokens in each sentence. So for example, the first batch (batch_size=14) will look like:\n",
    "```\n",
    "    [[2, 84, 3],\n",
    "     [2, 102, 3],     \n",
    "     [2, 63, 3],      \n",
    "     [2, 84, 21, 3]]\n",
    "```\n",
    "- We will then zero-pad all of the sentences in batch that are less than maximum length of the longest sentence in the batch to be the same length as the longest sentence:\n",
    "\n",
    "```\n",
    "    [[2, 84, 3, 0],\n",
    "     [2, 102, 3, 0],     \n",
    "     [2, 63, 3, 0],      \n",
    "     [2, 84, 21, 3]]\n",
    "```\n",
    "\n",
    "- Now we have a batch of dimension: $N x D$, where:\n",
    "    - $N$ is the number of sentences in the batch, and \n",
    "    - $D$ is the dimensionality (number of words) within a sentence.\n",
    "    - It is important to note that the $N$ and $D$ values will vary from batch to batch, but **MUST** be consistent within each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# THE FOLLOWING FUNCTION IS DEPRECATED\\n\\ndef get_batches(german, english, b_sz):\\n    batches = [[]]\\n    \\n    # For every sentence in the dataset, add it to a batch, based on the batch size\\n    # if the sentence + current length is not greater than the batch size, \\n    # then add it to the batch otherwise fill the current batch\\n    for sent in german:\\n        cur_len = 0\\n        for b in batches[-1]:\\n            cur_len += len(b)\\n    \\n        if (cur_len + len(sent)) <= b_sz: \\n            batches[-1].append(sent)\\n        else:\\n            batches.append([])        \\n            batches[-1].append(sent)\\n    \\n    # For every batch within the entire set of batches, add padding to the sentences\\n    # that are less than the length of the longest sentence within the each batch.\\n    for b in batches:\\n        max_len = len(max(b, key=len))\\n        \\n        for sent in b:\\n            dif = max_len - len(sent)\\n            if dif > 0:\\n                pad_list = 0 * dif\\n                sent.append(pad_list)\\n        \\n    return batches\\n\\n\\n\\nbatches = get_batches(german['train'], b_sz=20)\\n\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# THE FOLLOWING FUNCTION IS DEPRECATED\n",
    "\n",
    "def get_batches(german, english, b_sz):\n",
    "    batches = [[]]\n",
    "    \n",
    "    # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "    # if the sentence + current length is not greater than the batch size, \n",
    "    # then add it to the batch otherwise fill the current batch\n",
    "    for sent in german:\n",
    "        cur_len = 0\n",
    "        for b in batches[-1]:\n",
    "            cur_len += len(b)\n",
    "    \n",
    "        if (cur_len + len(sent)) <= b_sz: \n",
    "            batches[-1].append(sent)\n",
    "        else:\n",
    "            batches.append([])        \n",
    "            batches[-1].append(sent)\n",
    "    \n",
    "    # For every batch within the entire set of batches, add padding to the sentences\n",
    "    # that are less than the length of the longest sentence within the each batch.\n",
    "    for b in batches:\n",
    "        max_len = len(max(b, key=len))\n",
    "        \n",
    "        for sent in b:\n",
    "            dif = max_len - len(sent)\n",
    "            if dif > 0:\n",
    "                pad_list = 0 * dif\n",
    "                sent.append(pad_list)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "\n",
    "\n",
    "batches = get_batches(german['train'], b_sz=20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2, 865,   3], device='cuda:0')\n",
      "y tensor([0], device='cuda:0')\n",
      "x tensor([  2, 865,   3,   0], device='cuda:0')\n",
      "tensor([  2, 865,   3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def test(x):\n",
    "    y = [0]\n",
    "    y = torch.as_tensor(y).cuda()\n",
    "    print('y', y)\n",
    "    x = torch.cat((x, y), dim=0)\n",
    "    print('x', x)\n",
    "    \n",
    "    \n",
    "print(german['train'][0])\n",
    "test(german['train'][0])\n",
    "print(german['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 865, 3, 0]\n",
      "[2, 865, 3, 0]\n",
      "[2, 10408, 3, 0]\n",
      "[2, 115146, 3, 0]\n",
      "[2, 104464, 3, 0]\n",
      "[2, 61543, 3, 0]\n",
      "[2, 2858, 3, 0]\n",
      "[2, 54951, 3, 0]\n",
      "[2, 6944, 3, 0]\n",
      "[2, 865, 3, 0]\n",
      "[2, 865, 3, 0]\n",
      "[2, 115146, 3, 0]\n",
      "[2, 35921, 3, 0]\n",
      "[2, 15689, 3, 0]\n",
      "[2, 53131, 3, 0]\n",
      "[2, 3481, 3, 0]\n",
      "[2, 3211, 3, 0]\n",
      "[2, 865, 3, 0]\n",
      "[2, 3174, 3, 0]\n",
      "[2, 865, 11, 3]\n",
      "[2, 8403, 3180, 3]\n",
      "[2, 83114, 11, 3]\n",
      "[2, 865, 11, 3]\n",
      "[2, 865, 11, 3]\n",
      "[2, 865, 11, 3]\n",
      "[2, 3550, 11, 3]\n",
      "[2, 865, 11, 3]\n",
      "[2, 2302, 11, 3]\n",
      "[2, 865, 11, 3]\n"
     ]
    }
   ],
   "source": [
    "def get_batches(german, english, b_sz):\n",
    "    de_batches = [[]]\n",
    "    \n",
    "    # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "    # if the sentence + current length is not greater than the batch size, \n",
    "    # then add it to the batch otherwise fill the current batch\n",
    "    for sent in german:\n",
    "        cur_len = 0\n",
    "        for b in de_batches[-1]:\n",
    "            cur_len += len(b)\n",
    "    \n",
    "        if (cur_len + len(sent)) <= b_sz: \n",
    "            de_batches[-1].append(sent)\n",
    "        else:\n",
    "            de_batches.append([])        \n",
    "            de_batches[-1].append(sent)\n",
    "    \n",
    "    # For every batch within the entire set of batches, add padding to the sentences\n",
    "    # that are less than the length of the longest sentence within the each batch.\n",
    "    for b in de_batches:\n",
    "        max_len = len(max(b, key=len))\n",
    "        \n",
    "        for sent in b:\n",
    "            dif = max_len - len(sent)\n",
    "            if dif > 0:\n",
    "                pad_list = 0 * dif\n",
    "                sent.append(pad_list)\n",
    "    \n",
    "    en_batches = []\n",
    "    k=0\n",
    "    for i in range(len(de_batches)):\n",
    "        tmp_batch = [0]*len(de_batches[i])\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_batch[j] = english[k]\n",
    "            k+=1\n",
    "            \n",
    "        en_batches.append(tmp_batch)\n",
    "        \n",
    "    batches = []\n",
    "    for i in range(len(de_batches)):\n",
    "        dict_batch = []\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_dict = {\"source\": de_batches[i][j],\n",
    "                       \"target\": en_batches[i][j]}\n",
    "            dict_batch.append(tmp_dict)\n",
    "        batches.append(dict_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "test_batches = get_batches(german['train'], english['train'], b_sz=100)\n",
    "\n",
    "for i in range(len(test_batches[0])):\n",
    "    print(test_batches[0][i]['source'])\n",
    "\n",
    "# print(\"load the source and target sentences of the 3rd sentence within the 102nd batch:\")\n",
    "# print(\"Source:\", test_batches[102][3]['source'])\n",
    "# print(\"Target:\", test_batches[102][3]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the source and target sentences of the 3rd sentence within the 102nd batch:\n",
      "Source: tensor([  2, 864, 576,  11,   3], device='cuda:0')\n",
      "Target: tensor([   2, 3144,  131,  624,   11,    3], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-228fc120f5d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def get_batches(german, english, b_sz):\n",
    "    de_batches = [[]]\n",
    "    \n",
    "    # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "    # if the sentence + current length is not greater than the batch size, \n",
    "    # then add it to the batch otherwise fill the current batch\n",
    "    for sent in german:\n",
    "        cur_len = 0\n",
    "        for b in de_batches[-1]:\n",
    "            cur_len += len(b)\n",
    "    \n",
    "        if (cur_len + len(sent)) <= b_sz: \n",
    "            de_batches[-1].append(sent)\n",
    "        else:\n",
    "            de_batches.append([])        \n",
    "            de_batches[-1].append(sent)\n",
    "    \n",
    "    # For every batch within the entire set of batches, add padding to the sentences\n",
    "    # that are less than the length of the longest sentence within the each batch.\n",
    "    for b in de_batches:\n",
    "        max_len = len(max(b, key=len))\n",
    "        \n",
    "        for sent in b:\n",
    "            dif = max_len - len(sent)\n",
    "            if dif > 0:\n",
    "                pad_list = [0] * dif\n",
    "                pad_list = torch.as_tensor(pad_list).cuda()\n",
    "                sent.data = torch.cat((sent,pad_list), dim=0)\n",
    "#                 sent.data = x.clone()\n",
    "\n",
    "    en_batches = []\n",
    "    k=0\n",
    "    for i in range(len(de_batches)):\n",
    "        tmp_batch = [0]*len(de_batches[i])\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_batch[j] = english[k]\n",
    "            k+=1\n",
    "            \n",
    "        en_batches.append(tmp_batch)\n",
    "        \n",
    "    batches = []\n",
    "    for i in range(len(de_batches)):\n",
    "        dict_batch = []\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_dict = {\"source\": de_batches[i][j],\n",
    "                       \"target\": en_batches[i][j]}\n",
    "            dict_batch.append(tmp_dict)\n",
    "        batches.append(dict_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "test_batches = get_batches(german['train'], english['train'], b_sz=100)\n",
    "\n",
    "print(\"load the source and target sentences of the 3rd sentence within the 102nd batch:\")\n",
    "print(\"Source:\", test_batches[102][3]['source'])\n",
    "print(\"Target:\", test_batches[102][3]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> damit löschen sie zeit . <eos> \n",
      "<sos> so , in doing so , you 're erasing time . <eos> \n",
      "<sos> aaa : danke schön . <eos> \n",
      "<sos> aaa : thank you . <eos> \n"
     ]
    }
   ],
   "source": [
    "def print_sentence(sent, language):\n",
    "    if language == \"german\":\n",
    "        for w in sent:\n",
    "            print(german['idx2word'][w], end=' ')\n",
    "    elif language == \"english\":\n",
    "        for w in sent:\n",
    "            print(english['idx2word'][w], end=' ')\n",
    "    else:\n",
    "        print(\"Language should be either 'german' or 'english'\")\n",
    "        \n",
    "    print(\"\")\n",
    "\n",
    "print_sentence(test_batches[2984][0]['source'], language=\"german\")\n",
    "print_sentence(test_batches[2984][0]['target'], language=\"english\")\n",
    "\n",
    "del test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.source_vocab_size = params['source_vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.droprate = params['droprate']\n",
    "        self.layers = params['layers']\n",
    "        self.bidirectional = params['bidirectional']\n",
    "          \n",
    "        self.embeddings = nn.Embedding(self.source_vocab_size, self.d_emb)\n",
    "        self.dropout = nn.Dropout(self.droprate)\n",
    "        self.LSTM = nn.LSTM(self.d_emb, self.d_hid, num_layers=self.layers, dropout=self.droprate, bidirectional=self.bidirectional)\n",
    "        \n",
    "    def forward(self, source):\n",
    "        embed = self.dropout(self.embeddings(source))\n",
    "        outputs, (hidden, cell) = self.LSTM(embed)\n",
    "        \n",
    "        return hidden, cell        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.target_vocab_size = params['target_vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.droprate = params['droprate']\n",
    "        self.layers = params['layers']\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.source_vocab_size, self.d_emb)\n",
    "        self.dropout = nn.Dropout(self.droprate)\n",
    "        self.LSTM = nn.LSTM(d_emb, d_hid, n_layers=layers, dropout=self.droprate)\n",
    "        self.out = nn.Linear(d_hid, target_vocab_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        inp = inp.unsqueeze(0)\n",
    "        \n",
    "        embed = self.dropout(self.embeddings(inp))\n",
    "        \n",
    "        output, (hidden, cell) = self.LSTM(embed, (hidden, cell))\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(english, german, params, net):\n",
    "    batches = get_batches()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify():\n",
    "    \n",
    "    return\n",
    "\n",
    "batchify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
