{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# from torchtext.datasets import TranslationDataset, Multi30k\n",
    "# from torchtext.data import Field, BucketIterator\n",
    "# import spacy\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "with open(path + 'english_no_pad_sorted_5k.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'german_no_pad_sorted_5k.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(german['train'])):\n",
    "    german['train'][i] = torch.LongTensor(german['train'][i]).to(device)\n",
    "    english['train'][i] = torch.LongTensor(english['train'][i]).to(device)\n",
    "    \n",
    "for i in range(len(german['dev'])):\n",
    "    german['dev'][i] = torch.LongTensor(german['dev'][i]).to(device)\n",
    "    english['dev'][i] = torch.LongTensor(english['dev'][i]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-Based Batching Method\n",
    "\n",
    "Feed in the data from the preprocessed set, it should be sorted from shortest sentence to longest sentence without any padding on the sentences. The *get_batches* function will be used to create all of the batches for training, the output is a number of batches of varying dimensions, which is based on the batch size. Below is an example.\n",
    "\n",
    "#### Example, batch-size=14\n",
    "- Given the first few sentences from the dataset (sorted):\n",
    "    - [2, 84, 3]      (length 3)\n",
    "    - [2, 102, 3]     (length 3)\n",
    "    - [2, 63, 3]      (length 3)\n",
    "    - [2, 84, 21, 3]  (length 4)\n",
    "    - [2, 91, 123, 3] (length 4)\n",
    "    \n",
    "- We will fill up the batches based on the number of tokens in each sentence. So for example, the first batch (batch_size=14) will look like:\n",
    "```\n",
    "    [[2, 84, 3],\n",
    "     [2, 102, 3],     \n",
    "     [2, 63, 3],      \n",
    "     [2, 84, 21, 3]]\n",
    "```\n",
    "- We will then zero-pad all of the sentences in batch that are less than maximum length of the longest sentence in the batch to be the same length as the longest sentence:\n",
    "\n",
    "```\n",
    "    [[2, 84, 3, 0],\n",
    "     [2, 102, 3, 0],     \n",
    "     [2, 63, 3, 0],      \n",
    "     [2, 84, 21, 3]]\n",
    "```\n",
    "\n",
    "- Now we have a batch of dimension: $N x L$, where:\n",
    "    - $N$ is the number of sentences in the batch, and \n",
    "    - $L$ is the dimensionality (number of words) within a sentence.\n",
    "    - It is important to note that the $N$ and $L$ values will vary from batch to batch, but **MUST** be consistent within each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # THE FOLLOWING FUNCTION IS DEPRECATED\n",
    "\n",
    "# def get_batches(german, english, b_sz):\n",
    "#     batches = [[]]\n",
    "    \n",
    "#     # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "#     # if the sentence + current length is not greater than the batch size, \n",
    "#     # then add it to the batch otherwise fill the current batch\n",
    "#     for sent in german:\n",
    "#         cur_len = 0\n",
    "#         for b in batches[-1]:\n",
    "#             cur_len += len(b)\n",
    "    \n",
    "#         if (cur_len + len(sent)) <= b_sz: \n",
    "#             batches[-1].append(sent)\n",
    "#         else:\n",
    "#             batches.append([])        \n",
    "#             batches[-1].append(sent)\n",
    "    \n",
    "#     # For every batch within the entire set of batches, add padding to the sentences\n",
    "#     # that are less than the length of the longest sentence within the each batch.\n",
    "#     for b in batches:\n",
    "#         max_len = len(max(b, key=len))\n",
    "        \n",
    "#         for sent in b:\n",
    "#             dif = max_len - len(sent)\n",
    "#             if dif > 0:\n",
    "#                 pad_list = 0 * dif\n",
    "#                 sent.append(pad_list)\n",
    "        \n",
    "#     return batches\n",
    "\n",
    "\n",
    "\n",
    "# batches = get_batches(german['train'], b_sz=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # THE FOLLOWING FUNCTION IS DEPRECATED\n",
    "\n",
    "\n",
    "# def get_batches(german, english, b_sz):\n",
    "#     de_batches = [[]]\n",
    "    \n",
    "#     # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "#     # if the sentence + current length is not greater than the batch size, \n",
    "#     # then add it to the batch otherwise fill the current batch\n",
    "#     for sent in german:\n",
    "#         cur_len = 0\n",
    "#         for b in de_batches[-1]:\n",
    "#             cur_len += len(b)\n",
    "    \n",
    "#         if (cur_len + len(sent)) <= b_sz: \n",
    "#             de_batches[-1].append(sent)\n",
    "#         else:\n",
    "#             de_batches.append([])        \n",
    "#             de_batches[-1].append(sent)\n",
    "    \n",
    "#     # For every batch within the entire set of batches, add padding to the sentences\n",
    "#     # that are less than the length of the longest sentence within the each batch.\n",
    "#     for b in de_batches:\n",
    "#         max_len = len(max(b, key=len))\n",
    "        \n",
    "#         for sent in b:\n",
    "#             dif = max_len - len(sent)\n",
    "#             if dif > 0:\n",
    "#                 pad_list = 0 * dif\n",
    "#                 sent.append(pad_list)\n",
    "    \n",
    "#     en_batches = []\n",
    "#     k=0\n",
    "#     for i in range(len(de_batches)):\n",
    "#         tmp_batch = [0]*len(de_batches[i])\n",
    "#         for j in range(len(de_batches[i])):\n",
    "#             tmp_batch[j] = english[k]\n",
    "#             k+=1\n",
    "            \n",
    "#         en_batches.append(tmp_batch)\n",
    "        \n",
    "#     batches = []\n",
    "#     for i in range(len(de_batches)):\n",
    "#         dict_batch = []\n",
    "#         for j in range(len(de_batches[i])):\n",
    "#             tmp_dict = {\"source\": de_batches[i][j],\n",
    "#                        \"target\": en_batches[i][j]}\n",
    "#             dict_batch.append(tmp_dict)\n",
    "#         batches.append(dict_batch)\n",
    "        \n",
    "#     return batches\n",
    "\n",
    "# test_batches = get_batches(german['train'], english['train'], b_sz=100)\n",
    "\n",
    "# for i in range(len(test_batches[0])):\n",
    "#     print(test_batches[0][i]['source'])\n",
    "\n",
    "# # print(\"load the source and target sentences of the 3rd sentence within the 102nd batch:\")\n",
    "# # print(\"Source:\", test_batches[102][3]['source'])\n",
    "# # print(\"Target:\", test_batches[102][3]['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(german, english, b_sz):\n",
    "    de_batches = [[]]\n",
    "    \n",
    "    # For every sentence in the dataset, add it to a batch, based on the batch size\n",
    "    # if the sentence + current length is not greater than the batch size, \n",
    "    # then add it to the batch otherwise fill the current batch\n",
    "    for sent in german:\n",
    "        cur_len = 0\n",
    "        for b in de_batches[-1]:\n",
    "            cur_len += len(b)\n",
    "    \n",
    "        if (cur_len + len(sent)) <= b_sz: \n",
    "            de_batches[-1].append(sent)\n",
    "        else:\n",
    "            de_batches.append([])        \n",
    "            de_batches[-1].append(sent)\n",
    "    \n",
    "    # For every batch within the entire set of batches, add padding to the sentences\n",
    "    # that are less than the length of the longest sentence within the each batch.\n",
    "    for b in de_batches:\n",
    "        max_len = len(max(b, key=len))\n",
    "        \n",
    "        for sent in b:\n",
    "            dif = max_len - len(sent)\n",
    "            if dif > 0:\n",
    "                pad_list = [0] * dif\n",
    "                pad_list = torch.as_tensor(pad_list).to(device)\n",
    "                sent.data = torch.cat((sent,pad_list), dim=0)\n",
    "\n",
    "    # Within each batch, load the english sentence that corresponds to each\n",
    "    # german sentence within that batch to a new english batch.\n",
    "    en_batches = []\n",
    "    k=0\n",
    "    for i in range(len(de_batches)):\n",
    "        tmp_batch = [0]*len(de_batches[i])\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_batch[j] = english[k]\n",
    "            k+=1\n",
    "        en_batches.append(tmp_batch)\n",
    "        \n",
    "    # Within each english batch, pad all the sentences to be the same length as the \n",
    "    # maximum sentence within that batch.\n",
    "    for b in en_batches:\n",
    "        max_len = len(max(b, key=len))\n",
    "        \n",
    "        for sent in b:\n",
    "            dif = max_len - len(sent)\n",
    "            if dif > 0:\n",
    "                pad_list = [0] * dif\n",
    "                pad_list = torch.as_tensor(pad_list).to(device)\n",
    "                sent.data = torch.cat((sent,pad_list), dim=0)\n",
    "        \n",
    "    # Define the batches list, which contains an index for each batch. Within each index\n",
    "    # of the batches list is a batch. Each batch is of length N (number of sentences), \n",
    "    # Each index (0..N) within the batch is a dictionary containing the source and target\n",
    "    # sentence. Therefore, the batches list is a list of lists (essentially a 2D array), \n",
    "    # where each element in the list of lists/array is a dictionary containing the source\n",
    "    # and target tensor.\n",
    "    batches = []\n",
    "    for i in range(len(de_batches)):\n",
    "        dict_batch = []\n",
    "        for j in range(len(de_batches[i])):\n",
    "            tmp_dict = {\"source\": de_batches[i][j],\n",
    "                       \"target\": en_batches[i][j]}\n",
    "            dict_batch.append(tmp_dict)\n",
    "        batches.append(dict_batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# print(len(get_batches(german['train'],english['train'],1000)))\n",
    "\n",
    "# test_batches = get_batches(german['train'], english['train'], b_sz=100)\n",
    "\n",
    "# print(\"load the source and target sentences of the 3rd sentence within the 102nd batch:\")\n",
    "# print(\"Source:\", test_batches[102][3]['source'])\n",
    "# print(\"Target:\", test_batches[102][3]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function is used to retrieve the tensors for individual batches. The batchify() function will be used within the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(batch): # We need the batches to be of shape [sentence length, batch size]\n",
    "    source_sent_len = (len(batch[0]['source']))\n",
    "    target_sent_len = (len(batch[0]['target']))\n",
    "    batch_size = (len(batch))\n",
    "    \n",
    "    source = torch.empty((source_sent_len, batch_size)).long().to(device)    \n",
    "    target = torch.empty((target_sent_len, batch_size)).long().to(device)\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        source[:,i] = batch[i]['source']\n",
    "        target[:,i] = batch[i]['target']\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "\n",
    "# # # Example:\n",
    "# from time import time\n",
    "# t0 = time()\n",
    "# tr_batches = get_batches(german['train'], english['train'], b_sz=100)\n",
    "# print(time()-t0)\n",
    "\n",
    "# for idx, b in enumerate(tr_batches): # TRAINING LOOP\n",
    "#     source, target = batchify(b) # returns the source and target tensors that are in the correct \n",
    "#                                  # shape to push through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(sent, language):\n",
    "    if language == \"german\":\n",
    "        for w in sent:\n",
    "            print(german['idx2word'][w], end=' ')\n",
    "    elif language == \"english\":\n",
    "        for w in sent:\n",
    "            print(english['idx2word'][w], end=' ')\n",
    "    else:\n",
    "        print(\"Language should be either 'german' or 'english'\")\n",
    "        \n",
    "    print(\"\")\n",
    "\n",
    "# print_sentence(test_batches[2984][0]['source'], language=\"german\")\n",
    "# print_sentence(test_batches[2984][0]['target'], language=\"english\")\n",
    "\n",
    "# del test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.source_vocab_size = params['source_vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.droprate = params['droprate']\n",
    "        self.layers = params['layers']\n",
    "        self.bidirectional = params['bidirectional']\n",
    "          \n",
    "        self.embeddings = nn.Embedding(self.source_vocab_size, self.d_emb)\n",
    "        self.dropout = nn.Dropout(self.droprate)\n",
    "        self.LSTM = nn.LSTM(self.d_emb, self.d_hid, num_layers=self.layers, dropout=self.droprate, bidirectional=self.bidirectional)\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(self.d_hid*2, self.d_hid)\n",
    "        \n",
    "    def forward(self, source):\n",
    "        embed = self.dropout(self.embeddings(source))\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            outputs, hidden = self.LSTM(embed)\n",
    "\n",
    "            x = torch.cat((hidden[-2][-2,:,:], hidden[-1][-1,:,:]), dim=1)\n",
    "            hidden = torch.tanh(self.fc(x))\n",
    "            return outputs, hidden.unsqueeze(0)\n",
    "        else:\n",
    "            outputs, (hidden, cell) = self.LSTM(embed)\n",
    "            return hidden, cell        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.target_vocab_size = params['target_vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.droprate = params['droprate']\n",
    "        self.layers = params['layers']\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.target_vocab_size, self.d_emb)\n",
    "        self.dropout = nn.Dropout(self.droprate)\n",
    "        self.LSTM = nn.LSTM(self.d_emb, self.d_hid, num_layers=self.layers, dropout=self.droprate)\n",
    "        self.out = nn.Linear(self.d_hid, self.target_vocab_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        inp = inp.unsqueeze(0)\n",
    "        \n",
    "        embed = self.dropout(self.embeddings(inp))\n",
    "        \n",
    "        output, (hidden, cell) = self.LSTM(embed, (hidden, cell))\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBD(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(DecoderBD, self).__init__()\n",
    "        self.target_vocab_size = params['target_vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.droprate = params['droprate']\n",
    "        self.layers = params['layers']\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.target_vocab_size, self.d_emb)\n",
    "        self.dropout = nn.Dropout(self.droprate)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, num_layers=self.layers, dropout=self.droprate)\n",
    "        self.out = nn.Linear(self.d_hid, self.target_vocab_size)\n",
    "        \n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        inp = inp.unsqueeze(0)\n",
    "        \n",
    "        embed = self.dropout(self.embeddings(inp))\n",
    "\n",
    "        output, hidden = self.rnn(embed, hidden)\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bidirectional = encoder.bidirectional\n",
    "        \n",
    "        assert encoder.d_hid == decoder.d_hid, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.layers == decoder.layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "    \n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = target.shape[1]\n",
    "        sent_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.target_vocab_size\n",
    "        \n",
    "        outputs = torch.zeros(sent_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        inp = target[0,:]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            encoder_outputs, hidden = self.encoder(source)\n",
    "            for t in range(1, sent_len):\n",
    "                output, hidden = self.decoder(inp, hidden, encoder_outputs)\n",
    "                outputs[t] = output\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                top = output.max(1)[1]\n",
    "                inp = target[t] if teacher_force else top\n",
    "            \n",
    "        else:\n",
    "            hidden, cell = self.encoder(source)\n",
    "            for t in range(1, sent_len):\n",
    "                output, hidden = self.decoder(inp, hidden, cell)\n",
    "                outputs[t] = output\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                top = output.max(1)[1]\n",
    "                inp = target[t] if teacher_force else top\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, german, english, params):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    learningrate = params['learning_rate']\n",
    "    teacher_force = params['teacher_force']\n",
    "    batch_size = params['batch_size']\n",
    "    clip = params['clip']\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learningrate)\n",
    "    \n",
    "    batches = get_batches(german['train'], english['train'], batch_size)\n",
    "    print(\"Batch count:\", len(batches))\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    for idx, b in enumerate(batches):\n",
    "        source, target = batchify(b)\n",
    "\n",
    "        output = model(source, target, teacher_force)\n",
    "\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        target = target[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        if idx%500 == 0: print( \"batch:\", idx, \"loss:\", loss.item()) \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, german, english, params):\n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    batches = get_batches(german['dev'], english['dev'], batch_size)\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, b in enumerate(batches):\n",
    "            \n",
    "            source, target = batchify(b)\n",
    "            output = model(source, target, teacher_forcing_ratio=0)\n",
    "            \n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            target = target[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss/len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evan_\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch count: 2265\n",
      "batch: 0 loss: 10.92887020111084\n",
      "batch: 500 loss: 6.162031650543213\n",
      "batch: 1000 loss: 6.170685291290283\n",
      "batch: 1500 loss: 5.518272876739502\n",
      "batch: 2000 loss: 5.453644275665283\n",
      "Epoch: 01 | Time: 8m 13s\n",
      "\tTrain Loss: 5.755 | Train PPL: 315.803\n",
      "\t Val. Loss: 7.867 |  Val. PPL: 2609.765\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.533724784851074\n",
      "batch: 500 loss: 5.947695732116699\n",
      "batch: 1000 loss: 5.603154182434082\n",
      "batch: 1500 loss: 5.628115177154541\n",
      "batch: 2000 loss: 6.74653959274292\n",
      "Epoch: 02 | Time: 8m 33s\n",
      "\tTrain Loss: 5.594 | Train PPL: 268.751\n",
      "\t Val. Loss: 8.219 |  Val. PPL: 3709.259\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.870950222015381\n",
      "batch: 500 loss: 4.8925604820251465\n",
      "batch: 1000 loss: 5.914968013763428\n",
      "batch: 1500 loss: 5.715311050415039\n",
      "batch: 2000 loss: 5.583240032196045\n",
      "Epoch: 03 | Time: 8m 46s\n",
      "\tTrain Loss: 5.582 | Train PPL: 265.507\n",
      "\t Val. Loss: 8.866 |  Val. PPL: 7088.029\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.89659309387207\n",
      "batch: 500 loss: 6.319077968597412\n",
      "batch: 1000 loss: 5.793328762054443\n",
      "batch: 1500 loss: 5.479195594787598\n",
      "batch: 2000 loss: 5.258117198944092\n",
      "Epoch: 04 | Time: 8m 45s\n",
      "\tTrain Loss: 5.578 | Train PPL: 264.500\n",
      "\t Val. Loss: 8.379 |  Val. PPL: 4356.578\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 4.539527893066406\n",
      "batch: 500 loss: 5.590754985809326\n",
      "batch: 1000 loss: 5.186144828796387\n",
      "batch: 1500 loss: 6.055112361907959\n",
      "batch: 2000 loss: 5.679851055145264\n",
      "Epoch: 05 | Time: 8m 45s\n",
      "\tTrain Loss: 5.581 | Train PPL: 265.251\n",
      "\t Val. Loss: 8.191 |  Val. PPL: 3607.141\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.568534851074219\n",
      "batch: 500 loss: 5.806329727172852\n",
      "batch: 1000 loss: 5.32822322845459\n",
      "batch: 1500 loss: 4.108468532562256\n",
      "batch: 2000 loss: 4.9173197746276855\n",
      "Epoch: 06 | Time: 8m 45s\n",
      "\tTrain Loss: 5.586 | Train PPL: 266.550\n",
      "\t Val. Loss: 8.759 |  Val. PPL: 6366.290\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.411351203918457\n",
      "batch: 500 loss: 6.0412163734436035\n",
      "batch: 1000 loss: 5.19476318359375\n",
      "batch: 1500 loss: 5.6818671226501465\n",
      "batch: 2000 loss: 4.942022800445557\n",
      "Epoch: 07 | Time: 8m 45s\n",
      "\tTrain Loss: 5.586 | Train PPL: 266.694\n",
      "\t Val. Loss: 8.619 |  Val. PPL: 5538.337\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 4.8252997398376465\n",
      "batch: 500 loss: 5.260565280914307\n",
      "batch: 1000 loss: 6.342691898345947\n",
      "batch: 1500 loss: 5.22605562210083\n",
      "batch: 2000 loss: 5.291223526000977\n",
      "Epoch: 08 | Time: 8m 44s\n",
      "\tTrain Loss: 5.590 | Train PPL: 267.751\n",
      "\t Val. Loss: 8.550 |  Val. PPL: 5165.545\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.531671047210693\n",
      "batch: 500 loss: 5.756449222564697\n",
      "batch: 1000 loss: 6.220860004425049\n",
      "batch: 1500 loss: 5.4890851974487305\n",
      "batch: 2000 loss: 5.386316776275635\n",
      "Epoch: 09 | Time: 8m 45s\n",
      "\tTrain Loss: 5.602 | Train PPL: 270.958\n",
      "\t Val. Loss: 8.750 |  Val. PPL: 6310.146\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.102510929107666\n",
      "batch: 500 loss: 5.2147297859191895\n",
      "batch: 1000 loss: 5.658943176269531\n",
      "batch: 1500 loss: 6.179904937744141\n",
      "batch: 2000 loss: 5.54388952255249\n",
      "Epoch: 10 | Time: 8m 44s\n",
      "\tTrain Loss: 5.579 | Train PPL: 264.917\n",
      "\t Val. Loss: 8.730 |  Val. PPL: 6183.498\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 4.521787643432617\n",
      "batch: 500 loss: 5.705855846405029\n",
      "batch: 1000 loss: 5.640800952911377\n",
      "batch: 1500 loss: 5.9518961906433105\n",
      "batch: 2000 loss: 5.455354690551758\n",
      "Epoch: 11 | Time: 8m 44s\n",
      "\tTrain Loss: 5.587 | Train PPL: 267.040\n",
      "\t Val. Loss: 9.206 |  Val. PPL: 9959.521\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 3.9332568645477295\n",
      "batch: 500 loss: 5.724429130554199\n",
      "batch: 1000 loss: 5.973640441894531\n",
      "batch: 1500 loss: 6.064023971557617\n",
      "batch: 2000 loss: 5.339977741241455\n",
      "Epoch: 12 | Time: 8m 44s\n",
      "\tTrain Loss: 5.604 | Train PPL: 271.629\n",
      "\t Val. Loss: 8.777 |  Val. PPL: 6481.195\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.159708023071289\n",
      "batch: 500 loss: 5.117005348205566\n",
      "batch: 1000 loss: 5.4937028884887695\n",
      "batch: 1500 loss: 6.002436637878418\n",
      "batch: 2000 loss: 5.494737148284912\n",
      "Epoch: 13 | Time: 8m 44s\n",
      "\tTrain Loss: 5.607 | Train PPL: 272.233\n",
      "\t Val. Loss: 9.083 |  Val. PPL: 8803.654\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.1208319664001465\n",
      "batch: 500 loss: 5.314412593841553\n",
      "batch: 1000 loss: 5.380117416381836\n",
      "batch: 1500 loss: 6.1116461753845215\n",
      "batch: 2000 loss: 6.03633451461792\n",
      "Epoch: 14 | Time: 8m 44s\n",
      "\tTrain Loss: 5.634 | Train PPL: 279.898\n",
      "\t Val. Loss: 8.740 |  Val. PPL: 6246.517\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.334167957305908\n",
      "batch: 500 loss: 4.651638984680176\n",
      "batch: 1000 loss: 5.807459354400635\n",
      "batch: 1500 loss: 5.4548258781433105\n",
      "batch: 2000 loss: 5.876100063323975\n",
      "Epoch: 15 | Time: 8m 45s\n",
      "\tTrain Loss: 5.644 | Train PPL: 282.500\n",
      "\t Val. Loss: 9.076 |  Val. PPL: 8746.685\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.012665271759033\n",
      "batch: 500 loss: 5.274747371673584\n",
      "batch: 1000 loss: 5.895509719848633\n",
      "batch: 1500 loss: 5.294957160949707\n",
      "batch: 2000 loss: 5.638076305389404\n",
      "Epoch: 16 | Time: 8m 44s\n",
      "\tTrain Loss: 5.642 | Train PPL: 282.086\n",
      "\t Val. Loss: 8.482 |  Val. PPL: 4829.439\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 4.488572120666504\n",
      "batch: 500 loss: 5.6570024490356445\n",
      "batch: 1000 loss: 5.202325820922852\n",
      "batch: 1500 loss: 5.451930046081543\n",
      "batch: 2000 loss: 6.500185489654541\n",
      "Epoch: 17 | Time: 8m 45s\n",
      "\tTrain Loss: 5.638 | Train PPL: 280.888\n",
      "\t Val. Loss: 8.990 |  Val. PPL: 8024.348\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.664462566375732\n",
      "batch: 500 loss: 5.7583537101745605\n",
      "batch: 1000 loss: 5.738656044006348\n",
      "batch: 1500 loss: 5.979888439178467\n",
      "batch: 2000 loss: 6.212353706359863\n",
      "Epoch: 18 | Time: 8m 45s\n",
      "\tTrain Loss: 5.659 | Train PPL: 286.874\n",
      "\t Val. Loss: 8.929 |  Val. PPL: 7547.610\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 5.707465171813965\n",
      "batch: 500 loss: 5.355862617492676\n",
      "batch: 1000 loss: 5.675715446472168\n",
      "batch: 1500 loss: 4.745319366455078\n",
      "batch: 2000 loss: 5.80511999130249\n",
      "Epoch: 19 | Time: 8m 44s\n",
      "\tTrain Loss: 5.677 | Train PPL: 291.990\n",
      "\t Val. Loss: 9.177 |  Val. PPL: 9675.071\n",
      "Batch count: 2266\n",
      "batch: 0 loss: 4.671055793762207\n",
      "batch: 500 loss: 6.778548717498779\n",
      "batch: 1000 loss: 5.712469577789307\n",
      "batch: 1500 loss: 5.790652275085449\n",
      "batch: 2000 loss: 6.17276668548584\n",
      "Epoch: 20 | Time: 8m 45s\n",
      "\tTrain Loss: 5.696 | Train PPL: 297.692\n",
      "\t Val. Loss: 8.963 |  Val. PPL: 7811.755\n"
     ]
    }
   ],
   "source": [
    "tr_params = {}\n",
    "\n",
    "tr_params['batch_size'] = 200\n",
    "tr_params['learning_rate'] = 0.0025\n",
    "tr_params['teacher_force'] = 0.75\n",
    "tr_params['epochs'] = 20\n",
    "tr_params['clip'] = 1\n",
    "\n",
    "enc_params = {}\n",
    "dec_params = {}\n",
    "\n",
    "enc_params['source_vocab_size'] = len(german['idx2word'])\n",
    "enc_params['d_emb'] = 256\n",
    "enc_params['d_hid'] = 512\n",
    "enc_params['droprate'] = 0.5\n",
    "enc_params['layers'] = 1\n",
    "enc_params['bidirectional'] = True\n",
    "\n",
    "dec_params['target_vocab_size'] = len(english['idx2word'])\n",
    "dec_params['d_emb'] = 256\n",
    "dec_params['d_hid'] = 512\n",
    "dec_params['droprate'] = 0.5\n",
    "dec_params['layers'] = 1\n",
    "\n",
    "enc = Encoder(enc_params)\n",
    "\n",
    "if enc_params['bidirectional']:\n",
    "    dec = DecoderBD(dec_params)\n",
    "    model = seq2seq(enc, dec).to(device)\n",
    "else:\n",
    "    dec = Decoder(dec_params)\n",
    "    model = seq2seq(enc, dec).to(device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(tr_params['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, german, english, tr_params)\n",
    "    valid_loss = evaluate(model, german, english, tr_params)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'C:/Users/evan_/Documents/School/Graduate/Year 1/Spring/CS 690D/Project/Base Model/models/model00.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
