{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "with open(path + 'english_no_pad_sorted.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'german_no_pad_sorted.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get an average batch size of 64 Use a batch_size value of: 1328\n"
     ]
    }
   ],
   "source": [
    "def average_sentence_size(german, desired_batch_size=128):\n",
    "    summation = 0\n",
    "    for i in german['train']:\n",
    "        summation += len(i)\n",
    "    avg_tok_sent = summation/len(german['train'])\n",
    "    \n",
    "    print(\"To get an average batch size of\", desired_batch_size, \"Use a batch_size value of:\", int(desired_batch_size*avg_tok_sent))\n",
    "    \n",
    "average_sentence_size(german, desired_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(sent, language):\n",
    "    if language == \"german\":\n",
    "        for w in sent:\n",
    "            print(german['idx2word'][w], end=' ')\n",
    "    elif language == \"english\":\n",
    "        for w in sent:\n",
    "            print(english['idx2word'][w], end=' ')\n",
    "    else:\n",
    "        print(\"Language should be either 'german' or 'english'\")\n",
    "        \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.811434268951416\n"
     ]
    }
   ],
   "source": [
    "def get_batches(german, english, b_sz):\n",
    "    batches = [[]]\n",
    "    cur_batch = batches[-1]\n",
    "    cur_batch_len = 0\n",
    "        \n",
    "    for i in range(len(german)):\n",
    "        sent2sent = {\"source\": german[i],\n",
    "                     \"target\": english[i]}\n",
    "        \n",
    "        if cur_batch_len + len(sent2sent['source']) < b_sz:\n",
    "            cur_batch.append(sent2sent)\n",
    "            cur_batch_len += len(sent2sent['source'])\n",
    "        else:\n",
    "            batches.append([])\n",
    "            cur_batch = batches[-1]\n",
    "            cur_batch_len = 0 \n",
    "            cur_batch.append(sent2sent)\n",
    "            cur_batch_len += len(sent2sent['source'])\n",
    "\n",
    "            \n",
    "    for idx, b in enumerate(batches):\n",
    "        max_len_src = max([len(sent['source']) for sent in b])  \n",
    "        max_len_trg = max([len(sent['target']) for sent in b])\n",
    "\n",
    "        for sent in b:\n",
    "            dif_src = max_len_src - len(sent['source'])\n",
    "            dif_trg = max_len_trg - len(sent['target'])\n",
    "            \n",
    "            if dif_src > 0:\n",
    "                pad_list_src = [0 for d in range(dif_src)]\n",
    "                sent['source'].extend(pad_list_src)\n",
    "                \n",
    "            if dif_trg > 0:\n",
    "                pad_list_trg = [0 for d in range(dif_trg)]\n",
    "                sent['target'].extend(pad_list_trg)           \n",
    "            \n",
    "            sent['target'] = torch.tensor(sent['target']).cpu()\n",
    "            sent['source'] = torch.tensor(sent['source']).cpu()\n",
    "\n",
    "        \n",
    "    return batches\n",
    "t0 = time.time()\n",
    "batches = get_batches(german['train'], english['train'], 2650)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(batches)\n",
    "print(len(batches))\n",
    "for i in range (5):\n",
    "    print(len(batches[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(batch):\n",
    "    source_sent_len = len(batch[0]['source'])\n",
    "    target_sent_len = len(batch[0]['target'])\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    source = torch.empty((source_sent_len, batch_size)).cpu()    \n",
    "    target = torch.empty((target_sent_len, batch_size)).cpu()\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        source[:,i] = batch[i]['source']\n",
    "        target[:,i] = batch[i]['target']\n",
    "    \n",
    "    \n",
    "    return source, target\n",
    "\n",
    "for b in batches:\n",
    "    source, target = batchify(b)\n",
    "\n",
    "# results = []\n",
    "# for i in range(100):\n",
    "#     if i%10==0: print(i)\n",
    "#     random.shuffle(batches)\n",
    "\n",
    "#     t2 = time.time()\n",
    "#     for b in batches:\n",
    "#         source, target = batchify(b)\n",
    "#     results.append(time.time()-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000e+00, 2.0000e+00, 2.0000e+00,  ..., 2.0000e+00, 2.0000e+00,\n",
      "         2.0000e+00],\n",
      "        [3.0100e+02, 7.0000e+00, 6.1000e+01,  ..., 7.0000e+00, 3.3000e+01,\n",
      "         1.0600e+02],\n",
      "        [7.8400e+02, 8.0000e+00, 1.2000e+01,  ..., 8.0000e+00, 1.7820e+03,\n",
      "         1.2000e+01],\n",
      "        ...,\n",
      "        [1.7900e+02, 1.1000e+01, 2.0700e+02,  ..., 4.3600e+02, 4.1160e+03,\n",
      "         9.1282e+04],\n",
      "        [3.0000e+00, 3.0000e+00, 3.0000e+00,  ..., 1.1000e+01, 1.1000e+01,\n",
      "         1.1000e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0000e+00, 3.0000e+00,\n",
      "         3.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from building the tensors of the batches in the get_batches() function as .cpu() then using the tensors as input to batchify(), on the output of batchify() put them into .cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.291543083190918 seconds/set of batches, on average\n",
      "2.2153544425964355 3.837550640106201 min and max times (seconds)\n",
      "32768 bytes allocated\n",
      "1048576 bytes cached\n"
     ]
    }
   ],
   "source": [
    "print(sum(results)/len(results), \"seconds/set of batches, on average\")\n",
    "print(min(results), max(results), \"min and max times (seconds)\")\n",
    "\n",
    "print(torch.cuda.memory_allocated(device), \"bytes allocated\")\n",
    "print(torch.cuda.memory_cached(device), \"bytes cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from building the tensors in batchify(), as they are a list input for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5291345620155337 seconds/set of batches, on average\n",
      "3.422016143798828 5.263892889022827 min and max times (seconds)\n",
      "25088 bytes allocated\n",
      "1048576 bytes cached\n"
     ]
    }
   ],
   "source": [
    "print(sum(results)/len(results), \"seconds/set of batches, on average\")\n",
    "print(min(results), max(results), \"min and max times (seconds)\")\n",
    "\n",
    "print(torch.cuda.memory_allocated(device), \"bytes allocated\")\n",
    "print(torch.cuda.memory_cached(device), \"bytes cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE REASON THINGS ARE MESSED UP IS BECAUSE THE FUNCTION JUST KEEPS USING ALREADY WRITTEN DATA\n",
    "\n",
    "## ONLY CALL GET_BATCHES() ONCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another note: you might not need to put the batch on cuda at all, as it is just a tensor of indeces. Once the the batch goes through the embedding layer, it turns the indeces into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "\n",
    "with open(path + 'english_no_pad_sorted.pickle', 'rb') as handle:\n",
    "    english = pickle.load(handle)\n",
    "    \n",
    "with open(path + 'german_no_pad_sorted.pickle', 'rb') as handle:\n",
    "    german = pickle.load(handle)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_LEN = 3\n",
    "# MAX_LEN = 26\n",
    "# COUNT = 30\n",
    "# BUCKET_STEP = 3\n",
    "# BATCH_SIZE = 40\n",
    "\n",
    "# sentence_len = [random.randint(MIN_LEN, MAX_LEN) for i in range(COUNT)]\n",
    "# # values = [i for i in range(MIN_LEN, MAX_LEN, BUCKET_STEP)]\n",
    "# # buckets = [[] for i in values]\n",
    "\n",
    "# print(len(sentence_len))\n",
    "# print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_padded_tensor(batch):\n",
    "    max_len_src = max([len(sent['source']) for sent in batch])  \n",
    "    max_len_trg = max([len(sent['target']) for sent in batch])\n",
    "\n",
    "    for sent in batch:\n",
    "        dif_src = max_len_src - len(sent['source'])\n",
    "        dif_trg = max_len_trg - len(sent['target'])\n",
    "\n",
    "        if dif_src > 0:\n",
    "            pad_list_src = [0 for d in range(dif_src)]\n",
    "            sent['source'].extend(pad_list_src)\n",
    "\n",
    "        if dif_trg > 0:\n",
    "            pad_list_trg = [0 for d in range(dif_trg)]\n",
    "            sent['target'].extend(pad_list_trg)       \n",
    "    \n",
    "    source_sent_len = max_len_src\n",
    "    target_sent_len = max_len_trg\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    source = torch.empty((source_sent_len, batch_size)).long().cpu()    \n",
    "    target = torch.empty((target_sent_len, batch_size)).long().cpu()\n",
    "    \n",
    "#     print(batch[0]['target'])\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        source[:,i] = torch.tensor(batch[i]['source'])\n",
    "        target[:,i] = torch.tensor(batch[i]['target'])\n",
    "        \n",
    "    padded_tensor = {\"source\": source.to(device),\n",
    "                    \"target\": target.to(device)}\n",
    "    \n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "def bake_batches(de, en, batch_size=1300, min_len=3, max_len=768, bucket_step=3):\n",
    "    german = copy.deepcopy(de)\n",
    "    english = copy.deepcopy(en)\n",
    "    \n",
    "    buckets = [[] for i in range(min_len, max_len, bucket_step)]\n",
    "    bucket_lengths = [0 for i in buckets]\n",
    "    print(len(bucket_lengths))\n",
    "    batches = []\n",
    "    \n",
    "    # For every sentence in the dataset, find its corresponding bucket and put it in there, once the bucket\n",
    "    # hits the batch size, ship it off to the batches list\n",
    "    for i in range(len(german)):\n",
    "        sent2sent = {\"source\": german[i],\n",
    "                     \"target\": english[i]}\n",
    "        \n",
    "        # calculate the index of the buckets to put the sentence into, = len(Sentence) // Bucket_step - 1\n",
    "        idx = len(sent2sent['source'])//bucket_step - 1 \n",
    "\n",
    "        if bucket_lengths[idx] + len(sent2sent['source']) > batch_size:\n",
    "            batches.append(to_padded_tensor(buckets[idx][:]))\n",
    "            del buckets[idx][:]\n",
    "            buckets[idx].append(sent2sent)\n",
    "            bucket_lengths[idx] = len(sent2sent['source'])\n",
    "        else:\n",
    "            buckets[idx].append(sent2sent)\n",
    "            bucket_lengths[idx] += len(sent2sent['source'])\n",
    "            \n",
    "\n",
    "    # for any remaining buckets that did not get sent off, send them off to batches\n",
    "    for b in buckets:\n",
    "        if b: # if the list has any value in it\n",
    "            batches.append(to_padded_tensor(b[:]))\n",
    "            del b[:]\n",
    "    \n",
    "    return batches\n",
    "\n",
    "training_data = [[german['train'][i], english['train'][i]] for i in range(len(german))]\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "de_shuffled = [td[0] for td in training_data]\n",
    "en_shuffled = [td[1] for td in training_data]\n",
    "batches = bake_batches(de_shuffled, en_shuffled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': tensor([[    2,     2,     2,     2,     2],\n",
       "         [13146,  1581,  1581,  1581,  5624],\n",
       "         [   14,    14,    14,    14,    14],\n",
       "         [    3,     3,     3,     3,     3]], device='cuda:0'),\n",
       " 'target': tensor([[   2,    2,    2,    2,    2],\n",
       "         [9642,  658,  658,  658, 1270],\n",
       "         [  40,   23,   23,   23,  134],\n",
       "         [   3,   40,   40,   40,   40],\n",
       "         [   0,    3,    3,    3,    3]], device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for b in buckets:\n",
    "#     if b:\n",
    "#         print(b)\n",
    "        \n",
    "print(len(batches))\n",
    "total = 0\n",
    "for b in batches:\n",
    "    total += b['source'].shape[1]\n",
    "print(total)\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> erstaunliche antennen . <eos> \n",
      "<sos> we have amazing antennae . <eos> \n"
     ]
    }
   ],
   "source": [
    "print_sentence(batches[11][1]['source'], 'german')\n",
    "print_sentence(batches[11][1]['target'], 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9642, 40, 3]\n"
     ]
    }
   ],
   "source": [
    "print(english['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_zero(sentence):\n",
    "#     sentence = sentence.copy()\n",
    "#     sentence.append(0)\n",
    "    \n",
    "# print(german['train'][232])\n",
    "# add_zero(german['train'][232])\n",
    "# print(german['train'][232])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[german['train'][i], english['train'][i]] for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 735, 3541, 14, 3], [2, 710, 717, 40, 3]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(training_data)\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> man überkompensiert . <eos> \n",
      "<sos> you overcompensate . <eos> \n"
     ]
    }
   ],
   "source": [
    "print_sentence([2, 304, 8280, 14, 3], 'german')\n",
    "print_sentence([2, 23, 6385, 40, 3], 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_shuffled = [td[0] for td in training_data]\n",
    "en_shuffled = [td[1] for td in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 304, 8280, 14, 3] [2, 23, 6385, 40, 3]\n"
     ]
    }
   ],
   "source": [
    "print(de_shuffled[32], en_shuffled[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "114\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "115\n",
      "114\n",
      "114\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "training_data = [[german['train'][i], english['train'][i]] for i in range(50)]\n",
    "for i in range(20):\n",
    "    random.shuffle(training_data)\n",
    "\n",
    "    de_shuffled = [td[0] for td in training_data]\n",
    "    en_shuffled = [td[1] for td in training_data]\n",
    "    \n",
    "    batches = bake_batches(de_shuffled, en_shuffled, batch_size=20, max_len=20)\n",
    "    total = 0\n",
    "    for b in batches:\n",
    "        total += b['source'].shape[0]\n",
    "        total += b['source'].shape[1]\n",
    "    print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 9, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
