{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eIZimHHLhg2m",
        "colab_type": "code",
        "outputId": "5c62e624-963e-4061-93e8-c7893daf4238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('success!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 25.7MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 29.2MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 32.3MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 32.3MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 33.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 32.1MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 32.9MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 34.0MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 33.1MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 35.5MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 36.7MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 32.6MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 32.7MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 34.1MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 32.1MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 33.2MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 33.9MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 34.6MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 31.8MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 32.5MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 33.2MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 38.8MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 37.9MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 38.0MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 37.9MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 39.8MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 39.6MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 39.0MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 47.7MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 48.6MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 42.1MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 41.7MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 39.3MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 39.7MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 43.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 42.2MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 43.0MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 43.3MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 38.2MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 38.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 42.6MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 43.0MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 45.4MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 43.4MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 44.4MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 45.1MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 45.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 45.9MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 43.7MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 42.3MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 42.3MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 40.7MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 42.7MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 44.0MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 44.1MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 44.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 43.6MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 43.1MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 51.5MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 52.5MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 53.4MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 54.3MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 53.2MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 41.7MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 40.6MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 40.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 40.7MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 40.9MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 41.8MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 42.5MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 40.5MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 40.6MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 41.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 52.8MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 53.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 51.7MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 52.3MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 51.5MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 50.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 49.6MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 50.0MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 51.5MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 46.7MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 45.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 45.2MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 46.1MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.1MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 47.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 46.9MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 48.4MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 48.3MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 54.5MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 56.7MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 57.3MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hsuccess!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HqfI7uvjABqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from progressbar import ProgressBar, Percentage, Bar\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PSlj1sQAhkwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, pickle, os, sys, random, time\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "german = drive.CreateFile({'id': '17atuQ5p0LNY7nj4dcmlaFXD94dQV3NkB'})\n",
        "german.GetContentFile('./german_no_pad_no_sorted.pickle') \n",
        "with open('./german_no_pad_no_sorted.pickle', 'rb') as f_in:\n",
        "    german = pickle.load(f_in)\n",
        "\n",
        "english = drive.CreateFile({'id': '1NEoxrv_6GeoM0fPrpw4RWiyO19FaZd20'})\n",
        "english.GetContentFile('./english_no_pad_no_sorted.pickle') \n",
        "with open('./english_no_pad_no_sorted.pickle', 'rb') as f_in:\n",
        "    english = pickle.load(f_in)\n",
        "\n",
        "for i in range(len(german['train'])):\n",
        "    german['train'][i] = torch.LongTensor(german['train'][i]).cuda()\n",
        "    english['train'][i] = torch.LongTensor(english['train'][i]).cuda()\n",
        "    \n",
        "for i in range(len(german['dev'])):\n",
        "    german['dev'][i] = torch.LongTensor(german['dev'][i]).cuda()\n",
        "    english['dev'][i] = torch.LongTensor(english['dev'][i]).cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "60qsDsu_igen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(german, english, b_sz):\n",
        "    de_batches = [[]]\n",
        "    \n",
        "    # For every sentence in the dataset, add it to a batch, based on the batch size\n",
        "    # if the sentence + current length is not greater than the batch size, \n",
        "    # then add it to the batch otherwise fill the current batch\n",
        "    for sent in german:\n",
        "        cur_len = 0\n",
        "        for b in de_batches[-1]:\n",
        "            cur_len += len(b)\n",
        "    \n",
        "        if (cur_len + len(sent)) <= b_sz: \n",
        "            de_batches[-1].append(sent)\n",
        "        else:\n",
        "            de_batches.append([])        \n",
        "            de_batches[-1].append(sent)\n",
        "    \n",
        "    # For every batch within the entire set of batches, add padding to the sentences\n",
        "    # that are less than the length of the longest sentence within the each batch.\n",
        "    for b in de_batches:\n",
        "        max_len = len(max(b, key=len))\n",
        "        \n",
        "        for sent in b:\n",
        "            dif = max_len - len(sent)\n",
        "            if dif > 0:\n",
        "                pad_list = [0] * dif\n",
        "                pad_list = torch.as_tensor(pad_list).to(device)\n",
        "                sent.data = torch.cat((sent,pad_list), dim=0)\n",
        "\n",
        "    # Within each batch, load the english sentence that corresponds to each\n",
        "    # german sentence within that batch to a new english batch.\n",
        "    en_batches = []\n",
        "    k=0\n",
        "    for i in range(len(de_batches)):\n",
        "        tmp_batch = [0]*len(de_batches[i])\n",
        "        for j in range(len(de_batches[i])):\n",
        "            tmp_batch[j] = english[k]\n",
        "            k+=1\n",
        "        en_batches.append(tmp_batch)\n",
        "        \n",
        "    # Within each english batch, pad all the sentences to be the same length as the \n",
        "    # maximum sentence within that batch.\n",
        "    for b in en_batches:\n",
        "        max_len = len(max(b, key=len))\n",
        "        \n",
        "        for sent in b:\n",
        "            dif = max_len - len(sent)\n",
        "            if dif > 0:\n",
        "                pad_list = [0] * dif\n",
        "                pad_list = torch.as_tensor(pad_list).to(device)\n",
        "                sent.data = torch.cat((sent,pad_list), dim=0)\n",
        "        \n",
        "    # Define the batches list, which contains an index for each batch. Within each index\n",
        "    # of the batches list is a batch. Each batch is of length N (number of sentences), \n",
        "    # Each index (0..N) within the batch is a dictionary containing the source and target\n",
        "    # sentence. Therefore, the batches list is a list of lists (essentially a 2D array), \n",
        "    # where each element in the list of lists/array is a dictionary containing the source\n",
        "    # and target tensor.\n",
        "    batches = []\n",
        "    for i in range(len(de_batches)):\n",
        "        dict_batch = []\n",
        "        for j in range(len(de_batches[i])):\n",
        "            tmp_dict = {\"source\": de_batches[i][j],\n",
        "                       \"target\": en_batches[i][j]}\n",
        "            dict_batch.append(tmp_dict)\n",
        "        batches.append(dict_batch)\n",
        "        \n",
        "    return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tnBHPoapilnA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchify(batch): # We need the batches to be of shape [sentence length, batch size]\n",
        "    source_sent_len = (len(batch[0]['source']))\n",
        "    target_sent_len = (len(batch[0]['target']))\n",
        "    batch_size = (len(batch))\n",
        "    \n",
        "    source = torch.empty((source_sent_len, batch_size)).int().to(device)    \n",
        "    target = torch.empty((target_sent_len, batch_size)).int().to(device)\n",
        "    \n",
        "    for i in range(len(batch)):\n",
        "        source[:,i] = batch[i]['source']\n",
        "        target[:,i] = batch[i]['target']\n",
        "    \n",
        "    return source, target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4shlizI5wB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Example:\n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TlAZNRs07vh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
        "        \n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "            \n",
        "        #repeat encoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        \n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        \n",
        "        return output, hidden.squeeze(0)\n",
        "\n",
        "\n",
        "class Base(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.3):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).cuda()\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIaBG0G77w0A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(english_data, german_data, device):    \n",
        "    INPUT_DIM = len(german_data['idx2word'])\n",
        "    OUTPUT_DIM = len(english_data['idx2word'])\n",
        "    ENC_EMB_DIM = 256\n",
        "    DEC_EMB_DIM = 256\n",
        "    ENC_HID_DIM = 512\n",
        "    DEC_HID_DIM = 512\n",
        "    ENC_DROPOUT = 0.5\n",
        "    DEC_DROPOUT = 0.5\n",
        "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM,\n",
        "                  DEC_HID_DIM, ENC_DROPOUT)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM,\n",
        "                  DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "    model = Base(enc, dec, device).to(device)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N8VXlT32yy_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_to_drive(filename):\n",
        "    from googleapiclient.discovery import build\n",
        "    drive_service = build('drive', 'v3')\n",
        "\n",
        "    from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "    file_metadata = {\n",
        "      'name': filename\n",
        "    }\n",
        "    media = MediaFileUpload(filename, \n",
        "                            resumable=True)\n",
        "    created = drive_service.files().create(body=file_metadata,\n",
        "                                           media_body=media,\n",
        "                                           fields='id').execute()\n",
        "    print('File ID: {}'.format(created.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y84NojJQ73cq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(eng_train, eng_dev, de_train, de_dev, net, params):\n",
        "  \n",
        "    # padding is 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
        "    \n",
        "    #checkpoint = torch.load('checkpoint_3.pth')\n",
        "    #net.load_state_dict(checkpoint['state_dict'])\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    tr_batches = get_batches(german['train'], english['train'], b_sz=params['batch_size'])\n",
        "    print('batching done')\n",
        "    \n",
        "    \n",
        "    \n",
        "    for epoch in range(params['epochs']):\n",
        "        random.shuffle(tr_batches)\n",
        "        ep_loss = 0.\n",
        "        start_time = time.time()\n",
        "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(tr_batches)).start()\n",
        "        \n",
        "        \n",
        "        # for each batch, calculate loss and optimize model parameters            \n",
        "        i = 0\n",
        "        for b in tr_batches: # TRAINING LOOP\n",
        "            source, target = batchify(b)\n",
        "            preds = net(source.long(), target.long())\n",
        "            \n",
        "            preds = preds[0:].view(-1, preds.shape[-1])\n",
        "            eng_target = target[0:].view(-1)\n",
        "            loss = criterion(preds, eng_target.long())\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            ep_loss += loss\n",
        "            pbar.update(i+1)\n",
        "            i += 1\n",
        "        pbar.finish()\n",
        "        \n",
        "        checkpoint = {}\n",
        "        checkpoint['state_dict'] = net.state_dict()\n",
        "        checkpoint['optimizer'] = optimizer.state_dict()\n",
        "        checkpoint['epoch'] = epoch\n",
        "        file_name = 'checkpoint_{0}.pth'.format(epoch)        \n",
        "        torch.save(checkpoint, file_name)\n",
        "        write_to_drive(file_name)\n",
        "        #files.download(file_name)\n",
        "        \n",
        "        print('epoch: {0}, loss: {1}, time: {2}'.format(epoch, ep_loss, time.time()-start_time))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FLzQsZe8BWK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ff52e76f-7f6e-4ac8-ba96-a40d7fca7118"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = create_model(english, german, device)\n",
        "params = {}\n",
        "params['batch_size'] = 1000\n",
        "params['epochs'] = 10\n",
        "params['learning_rate'] = 0.001\n",
        "train(english['train'], english['dev'],german['train'], german['dev'], model, params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A%|                                                                         |"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batching done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 98%|######################################################################## |"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jo2yJNuqw4rH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = drive.CreateFile({'id': '1lhAkXEpQX9R2kCeduzpA2kP8ZV13gyyj'})\n",
        "d.GetContentFile('./checkpoint.pth') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dGEHvfDyGy_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def beam_search(probs, index, preds=[], end_index=3, beam_size=5):\n",
        "    probs = torch.permute(1,0,2) \n",
        "    # [batch_size, seq_len, embeddings]\n",
        "    soft = torch.softmax(dim=2)\n",
        "    if len(preds) = 0:\n",
        "        for i in range(beam_size):\n",
        "            temp = torch.Tensor([2])\n",
        "            preds.append(temp)\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sThAg-gvyUJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(net):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    eva_batches = get_batches(german['dev'], english['dev'], b_sz=1000)\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
        "    \n",
        "    checkpoint = torch.load('checkpoint.pth')\n",
        "    net.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    ep_loss = 0\n",
        "    i = 0\n",
        "    all_preds = []\n",
        "    for b in eva_batches: # TRAINING LOOP\n",
        "        \n",
        "        source, target = batchify(b)\n",
        "        preds = net(source.long(), target.long(), 0)\n",
        "        \n",
        "        temp = preds[0:].view(-1, preds.shape[-1])\n",
        "        eng_target = target[0:].view(-1)\n",
        "        loss = criterion(temp, eng_target.long())\n",
        "        preds = torch.softmax(preds, dim=2)\n",
        "        preds = torch.argmax(preds, dim=2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        ep_loss += loss\n",
        "        all_preds.append(preds)\n",
        "        i += 1\n",
        "    print(i)\n",
        "    return all_preds\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eW50bSueNmj",
        "colab_type": "code",
        "outputId": "3b32cfa1-537e-4f4c-b180-a14c7ec97dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = evaluate(model)\n",
        "eva_batches = get_batches(german['dev'], english['dev'], b_sz=1000)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G8vJ99b0Zltz",
        "colab_type": "code",
        "outputId": "d2e9c181-fa44-48e1-fd02-5634aed91247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "G56JqEknmuQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eng_ex = []\n",
        "for batch in a:\n",
        "    x,y = batch.size()\n",
        "    for i in range(y):\n",
        "      sentence = []\n",
        "      for idx in batch[:,i]:\n",
        "        sentence.append(english['idx2word'][idx.item()])\n",
        "        if idx.item() == 3:\n",
        "           break\n",
        "      eng_ex.append((' '.join(sentence[1:-1])))\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XT5ZBKlcge32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = a[batch_num]\n",
        "x,y = t.size()\n",
        "eng_ex = []\n",
        "for i in range(y):\n",
        "    sentence = []\n",
        "    for idx in t[:,i]:\n",
        "        sentence.append(english['idx2word'][idx.item()])\n",
        "        if idx.item() == 3:\n",
        "           break\n",
        "    eng_ex.append((' '.join(sentence[1:-1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5F5v7Dzhnnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eng_t = []\n",
        "for j in range(172):\n",
        "  _, target = batchify(eva_batches[j])\n",
        "  _, b = target.size()\n",
        "  for i in range(b):\n",
        "      sentence = []\n",
        "      for idx in target[:,i]:\n",
        "          sentence.append(english['idx2word'][idx.item()])\n",
        "          if idx.item() == 3:\n",
        "             break\n",
        "      eng_t.append((' '.join(sentence[1:-1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LWVYZV2hCSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(len(eng_ex)):\n",
        "#     print(\"Model: \", eng_ex[i], \" Target: \", eng_t[i])\n",
        "with open('real.out', 'w') as f:\n",
        "    for line in eng_t:\n",
        "        f.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9n6Z5qka2ST",
        "colab_type": "code",
        "outputId": "a915d1f5-d425-4c2d-ec50-ba9ab0cce09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "!pip install sacrebleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 147295, done.\u001b[K\n",
            "remote: Total 147295 (delta 0), reused 0 (delta 0), pack-reused 147295\u001b[K\n",
            "Receiving objects: 100% (147295/147295), 129.69 MiB | 12.70 MiB/s, done.\n",
            "Resolving deltas: 100% (113833/113833), done.\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/12/5b/7196b11bca204cb6ca9000b5dc910e809081f224c73ef28e9991080e4e51/sacrebleu-1.3.1.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/c0/fb/1c7f9b3a71f64cdf86291cc645596f71746807bf2f72b3c1dd\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "um6ezQR_a5pz",
        "colab_type": "code",
        "outputId": "016810f6-b120-4496-9d47-a0ed40df3327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "#!/bin/bash\n",
        "\n",
        "# This is a reference to the gold translations from the dev set\n",
        "REFERENCE_FILE=\"real.out\"\n",
        "\n",
        "# XXX: Change the following line to point to your model's output!\n",
        "TRANSLATED_FILE=\"model.out\"\n",
        "\n",
        "# The model output is expected to be in a tokenized form. Note, that if you\n",
        "# tokenized your inputs to the model, then simply joined each output token with\n",
        "# whitespace you should get tokenized outputs from your model.\n",
        "# i.e. each output token is separate by whitespace\n",
        "# e.g. \"My model 's output is interesting .\"\n",
        "perl \"mosesdecoder/scripts/tokenizer/detokenizer.perl\" -l en < \"$TRANSLATED_FILE\" > \"$TRANSLATED_FILE.detok\"\n",
        "\n",
        "PARAMS=(\"-tok\" \"intl\" \"-l\" \"de-en\" \"$REFERENCE_FILE\")\n",
        "sacrebleu \"${PARAMS[@]}\" < \"$TRANSLATED_FILE.detok\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: en\n",
            "BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+tok.intl+version.1.3.1 = 9.5 42.4/15.0/6.1/2.6 (BP = 0.954 ratio = 0.955 hyp_len = 155769 ref_len = 163089)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "NYmZhWNycWD3",
        "colab_type": "code",
        "outputId": "c9e4d9f3-bd98-4347-fc02-67639a0c5e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 25 13:52:53 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}